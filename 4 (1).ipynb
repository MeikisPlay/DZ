{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loEl0p9gxaHT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from gensim import corpora, models\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
        "import re\n",
        "from numpy import argmax\n",
        "\n",
        "df = pd.read_csv(\"/content/cleaned_museums.csv\")\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(nltk.corpus.stopwords.words('russian'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return [w for w in tokens if w not in stop_words and len(w) > 2]\n",
        "\n",
        "df['processed_text'] = df['Description'].apply(preprocess_text)\n",
        "\n",
        "vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
        "doc_term_matrix = vectorizer.fit_transform(df['Description'])\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda.fit(doc_term_matrix)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
        "    print(f\"Тема {topic_idx + 1}: {top_words}\\n\")\n",
        "\n",
        "vectorizer_lsa = TfidfVectorizer(max_df=0.95, min_df=2)\n",
        "doc_term_matrix_lsa = vectorizer_lsa.fit_transform(df['Description'])\n",
        "\n",
        "lsa = TruncatedSVD(n_components=5, random_state=42)\n",
        "lsa.fit(doc_term_matrix_lsa)\n",
        "\n",
        "feature_names_lsa = vectorizer_lsa.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lsa.components_):\n",
        "    top_words = [feature_names_lsa[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
        "    print(f\"Тема LSA {topic_idx + 1}: {top_words}\\n\")\n",
        "\n",
        "nmf = NMF(n_components=5, random_state=42)\n",
        "nmf.fit(doc_term_matrix)\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"ru_core_news_sm\")\n",
        "except OSError:\n",
        "    print(\"Модель spaCy 'ru_core_news_sm' не найдена. Пожалуйста, загрузите её.\")\n",
        "    exit()\n",
        "\n",
        "def extract_entities(text):\n",
        "    entities = {}\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        entities.setdefault(ent.label_, []).append(ent.text)\n",
        "    return entities\n",
        "\n",
        "df['entities'] = df['Description'].apply(extract_entities)\n",
        "\n",
        "topic_probabilities = lda.transform(doc_term_matrix)\n",
        "for i in range(lda.n_components):\n",
        "    top_words = [feature_names[j] for j in lda.components_[i].argsort()[:-10 - 1:-1]]\n",
        "    print(f\"\\nТема LDA {i+1}: {top_words}\")\n",
        "\n",
        "    topic_indices = argmax(topic_probabilities, axis=1) == i\n",
        "    entities_in_topic = []\n",
        "    for index in df[topic_indices].index:\n",
        "        entities_in_topic.extend(df['entities'][index])\n",
        "    print(\"Сущности в теме:\", entities_in_topic)"
      ]
    }
  ]
}
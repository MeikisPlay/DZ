{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgD92SYl6i16"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Загрузка необходимых ресурсов NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Загрузка данных\n",
        "df = pd.read_csv(\"cleaned_museums.csv\", encoding='utf-8')\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
        "    return text\n",
        "\n",
        "# TF-IDF векторизация для униграмм и биграмм\n",
        "stop_words = stopwords.words('russian')\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=1, stop_words=stop_words)\n",
        "tfidf_matrix = vectorizer.fit_transform(df['Description'].apply(clean_text))\n",
        "\n",
        "# Создание модели Word2Vec\n",
        "sentences = [word_tokenize(clean_text(text)) for text in df['Description']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "# Векторизация с tf-idf весами\n",
        "tfidf_word2vec = np.zeros((len(sentences), 100))\n",
        "for i, sentence in enumerate(sentences):\n",
        "    sentence_vec = np.zeros(100)\n",
        "    weighted_sum = 0\n",
        "    for word in sentence:\n",
        "        if word in word2vec_model.wv and word in vectorizer.vocabulary_:\n",
        "            tfidf_score = tfidf_matrix[i, vectorizer.vocabulary_[word]]\n",
        "            sentence_vec += word2vec_model.wv[word] * tfidf_score\n",
        "            weighted_sum += tfidf_score\n",
        "    if weighted_sum > 0:\n",
        "        sentence_vec /= weighted_sum\n",
        "    tfidf_word2vec[i, :] = sentence_vec\n",
        "\n",
        "# K-Means кластеризация\n",
        "inertia = []\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(tfidf_word2vec)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Построение графика метода локтя\n",
        "plt.plot(range(2, 11), inertia, marker='o')\n",
        "plt.xlabel('Количество кластеров (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Метод локтя')\n",
        "plt.show()\n",
        "\n",
        "# Определение оптимального количества кластеров и применение K-Means\n",
        "optimal_k = 3\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(tfidf_word2vec)\n",
        "\n",
        "# Визуализация кластеров с помощью t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_results = tsne.fit_transform(tfidf_word2vec)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(optimal_k):\n",
        "    plt.scatter(tsne_results[df['cluster'] == i, 0], tsne_results[df['cluster'] == i, 1], label=f'Cluster {i}')\n",
        "plt.title('Визуализация кластеров с помощью t-SNE')\n",
        "plt.xlabel('t-SNE компонент 1')\n",
        "plt.ylabel('t-SNE компонент 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}